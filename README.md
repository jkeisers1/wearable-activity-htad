# ğŸƒâ€â™‚ï¸ wearable-htad

How do we infer *what a person is doing* from the sensors they carry?

This project explores **human activity recognition** using **wearable accelerometers** and **microphone audio** data from the [HTAD dataset](https://example-link).  
It fuses both signals to classify activities such as *walking, standing, sitting,* and *lying* using interpretable, classical machine-learning models.

The goal was twofold:
1. Build a **reproducible open-source pipeline** (data â†’ features â†’ models â†’ metrics).
2. Understand how **sensor fusion** affects generalization across users.

I developed the project as part of a broader exploration of health-data analytics â€” demonstrating how to turn messy real-world signals into structured, validated insight.
